---
title: "Project603"
author: "Ashish Kondaka"
date: "`r Sys.Date()`"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}

install.packages("xgboost")
library(xgboost)
#libraries
library(tidyverse)
library(caret)
library(randomForest)
library(tree)
library(class)
library(kknn)


Student_performance_data <- read_csv("/Users/ashishkondaka/Downloads/Student_performance_data _.csv")

# Convert categorical variables to factors
Student_performance_data <- Student_performance_data %>%
  mutate(Gender = as.factor(Gender),
         Ethnicity = as.factor(Ethnicity),
         ParentalEducation = as.factor(ParentalEducation),
         Tutoring = as.factor(Tutoring),
         ParentalSupport = as.factor(ParentalSupport),
         Extracurricular = as.factor(Extracurricular),
         Sports = as.factor(Sports),
         Music = as.factor(Music),
         Volunteering = as.factor(Volunteering),
         GradeClass = as.factor(GradeClass))

# Split the data into training (50%) and testing (50%) sets 
set.seed(12345)
inTrain <- sample(nrow(Student_performance_data), 0.5 * nrow(Student_performance_data))
StudentTrain <- Student_performance_data[inTrain,]
StudentTest <- Student_performance_data[-inTrain,]

# Visualize the distribution of GradeClass
ggplot(Student_performance_data, aes(x = GradeClass)) + geom_bar() + theme_minimal() + ggtitle("Distribution of Grade Classes")

# Visualize relationships between GPA and potential predictors
ggplot(Student_performance_data, aes(x = StudyTimeWeekly, y = GPA)) + geom_point() + theme_minimal() + ggtitle("Study Time Weekly vs GPA")

ggplot(Student_performance_data, aes(x = Age, y = GPA)) + geom_point() + theme_minimal() + ggtitle("Age vs GPA")

ggplot(Student_performance_data, aes(x = Gender, y = GPA)) + geom_boxplot() + theme_minimal() + ggtitle("Gender vs GPA")



# KNN
knn_pred_class <- knn(train = StudentTrain[ , -15], test = StudentTest[ , -15], cl = StudentTrain$GradeClass, k = 3)

# consistent levels
knn_pred_class <- factor(knn_pred_class, levels = levels(StudentTest$GradeClass))

# Confusion Matrix
knn_conf_matrix <- confusionMatrix(knn_pred_class, StudentTest$GradeClass)
print(knn_conf_matrix)

# Binary Sensitivity and Specificity Calculation
knn_pred_class_binary <- knn_pred_class == positive_class

sensitivity_knn <- sensitivity(as.factor(knn_pred_class_binary), as.factor(StudentTest_binary), positive = "TRUE")
specificity_knn <- specificity(as.factor(knn_pred_class_binary), as.factor(StudentTest_binary), positive = "TRUE")

sensitivity_knn
specificity_knn

# Support Vector Machine (SVM)
#SVMs are powerful for high-dimensional data classification.

library(e1071)
svm_model <- svm(GradeClass ~ ., data = StudentTrain, kernel = "radial", probability = TRUE)
svm_pred_class <- predict(svm_model, StudentTest, probability = TRUE)
confusionMatrix(svm_pred_class, StudentTest$GradeClass)

# Prepare the data
train_matrix <- xgb.DMatrix(data = as.matrix(StudentTrain[, -which(names(StudentTrain) == "GradeClass")]), label = as.numeric(StudentTrain$GradeClass) - 1)
test_matrix <- xgb.DMatrix(data = as.matrix(StudentTest[, -which(names(StudentTest) == "GradeClass")]), label = as.numeric(StudentTest$GradeClass) - 1)


# Ensure categorical variables are converted to factors
StudentTrain <- StudentTrain %>% mutate(across(where(is.character), as.factor))
StudentTest <- StudentTest %>% mutate(across(where(is.character), as.factor))

# One-hot encode categorical variables
dmy <- dummyVars(" ~ .", data = StudentTrain)
train_transformed <- data.frame(predict(dmy, newdata = StudentTrain))
test_transformed <- data.frame(predict(dmy, newdata = StudentTest))

# Convert the target variable to numeric
train_transformed$GradeClass <- as.numeric(StudentTrain$GradeClass) - 1
test_transformed$GradeClass <- as.numeric(StudentTest$GradeClass) - 1

# Create DMatrix for XGBoost
train_matrix <- xgb.DMatrix(data = as.matrix(train_transformed[, -which(names(train_transformed) == "GradeClass")]), label = train_transformed$GradeClass)
test_matrix <- xgb.DMatrix(data = as.matrix(test_transformed[, -which(names(test_transformed) == "GradeClass")]), label = test_transformed$GradeClass)

# Set parameters for multi-class classification
params <- list(
  objective = "multi:softmax",
  num_class = length(unique(train_transformed$GradeClass)),
  eval_metric = "merror"
)

# Train the XGBoost model
xgb_model <- xgb.train(params, train_matrix, nrounds = 100)

# Make predictions on the test set
xgb_pred_class <- predict(xgb_model, test_matrix)

# Convert predictions to factor with correct levels
xgb_pred_class <- factor(xgb_pred_class, levels = 0:(length(unique(train_transformed$GradeClass)) - 1), labels = levels(StudentTrain$GradeClass))

# Evaluate the model
confusion_matrix_xgb <- confusionMatrix(xgb_pred_class, factor(StudentTest$GradeClass, levels = levels(StudentTrain$GradeClass)))
print(confusion_matrix_xgb)

# Calculate binary sensitivity and specificity if needed
positive_class <- "1" # Adjust this according to your actual positive class label
StudentTest_binary_xgb <- StudentTest$GradeClass == positive_class
xgb_pred_class_binary <- xgb_pred_class == positive_class

sensitivity_xgb <- sensitivity(as.factor(xgb_pred_class_binary), as.factor(StudentTest_binary_xgb), positive = "TRUE")
specificity_xgb <- specificity(as.factor(xgb_pred_class_binary), as.factor(StudentTest_binary_xgb), positive = "TRUE")

sensitivity_xgb
specificity_xgb

## using GBM
# Load necessary libraries
install.packages("gbm")
library(gbm)
library(caret)
library(tidyverse)

# Function to train GBM models in a one-vs-all approach
train_gbm_one_vs_all <- function(data, target_var, classes, n_trees = 100, interaction_depth = 3, shrinkage = 0.1) {
  models <- list()
  for (class in classes) {
    # Create a binary target variable for the current class
    data$binary_target <- as.numeric(data[[target_var]] == class)
    
    # Train the GBM model
    model <- gbm(binary_target ~ . -binary_target, data = data, distribution = "bernoulli",
                 n.trees = n_trees, interaction.depth = interaction_depth, shrinkage = shrinkage, cv.folds = 5)
    
    # Save the model
    models[[class]] <- model
  }
  return(models)
}

# Function to predict using the one-vs-all models
predict_gbm_one_vs_all <- function(models, data) {
  predictions <- data.frame(matrix(ncol = length(models), nrow = nrow(data)))
  colnames(predictions) <- names(models)
  
  for (class in names(models)) {
    model <- models[[class]]
    predictions[[class]] <- predict(model, newdata = data, n.trees = gbm.perf(model, method = "cv"), type = "response")
  }
  
  # Assign the class with the highest probability
  predicted_classes <- colnames(predictions)[apply(predictions, 1, which.max)]
  return(predicted_classes)
}

# Prepare the data
classes <- levels(StudentTrain$GradeClass)
StudentTrain <- StudentTrain %>% mutate(GradeClass = as.character(GradeClass))
StudentTest <- StudentTest %>% mutate(GradeClass = as.character(GradeClass))

# Train the one-vs-all GBM models
gbm_models <- train_gbm_one_vs_all(StudentTrain, target_var = "GradeClass", classes = classes)

# Predict on the test set
gbm_pred_class <- predict_gbm_one_vs_all(gbm_models, StudentTest)

# Convert predictions to factor with correct levels
gbm_pred_class <- factor(gbm_pred_class, levels = classes)

# Evaluate the model
confusion_matrix <- confusionMatrix(gbm_pred_class, factor(StudentTest$GradeClass, levels = classes))
print(confusion_matrix)

# Calculate binary sensitivity and specificity if needed
positive_class <- "1" # Adjust this according to your actual positive class label
StudentTest_binary <- StudentTest$GradeClass == positive_class
gbm_pred_class_binary <- gbm_pred_class == positive_class

sensitivity_gbm <- sensitivity(as.factor(gbm_pred_class_binary), as.factor(StudentTest_binary), positive = "TRUE")
specificity_gbm <- specificity(as.factor(gbm_pred_class_binary), as.factor(StudentTest_binary), positive = "TRUE")

sensitivity_gbm
specificity_gbm
##
#Executive Summary (sample)
#This project aimed to identify the key factors predicting high school students' academic performance, measured by GradeClass, using logistic regression, decision tree, and KNN models. Our key findings are as follows:
#- Finding 1: Study Time: Increased study time weekly is positively correlated with higher GradeClass, suggesting that consistent study habits are crucial for academic success.
#- Finding 2: Parental Education: Higher levels of parental education are associated with higher GradeClass, indicating the influence of parental background on student performance.
#- Finding 3: Gender: There are significant differences in GradeClass between male and female students.
#- Finding 4: Extracurricular Activities: Participation in extracurricular activities positively correlates with GradeClass, highlighting the benefits of a well-rounded education.
#- Finding 5: Absences: Higher absenteeism negatively correlates with GradeClass, emphasizing the importance of regular attendance.

#Based on these findings, we recommend improving study habits, encouraging parental involvement in education, promoting extracurricular activities, and addressing absenteeism to enhance student performance.

```
```

