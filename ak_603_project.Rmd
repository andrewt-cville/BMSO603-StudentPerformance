---
title: "Project603"
author: "Ashish Kondaka"
date: "`r Sys.Date()`"
output: word_document
---
title: "Project603"
author: "Ashish Kondaka"
date: "`r Sys.Date()`"
output: word_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Install and load necessary libraries
install.packages("xgboost")
install.packages("gbm")
install.packages("mltools")
install.packages("e1071")
install.packages("caret")
install.packages("tidyverse")
install.packages("class")
library(xgboost)
library(gbm)
library(tidyverse)
library(caret)
library(class)
library(e1071)
library(mltools)

Student_performance_data <- read_csv("/Users/ashishkondaka/Downloads/Student_performance_data _.csv")

# Convert categorical variables to factors
Student_performance_data <- Student_performance_data %>%
  mutate(Gender = as.factor(Gender),
         Ethnicity = as.factor(Ethnicity),
         ParentalEducation = as.factor(ParentalEducation),
         Tutoring = as.factor(Tutoring),
         ParentalSupport = as.factor(ParentalSupport),
         Extracurricular = as.factor(Extracurricular),
         Sports = as.factor(Sports),
         Music = as.factor(Music),
         Volunteering = as.factor(Volunteering),
         GradeClass = as.factor(GradeClass))

# Split the data into training (50%) and testing (50%) sets 
set.seed(12345)
inTrain <- sample(nrow(Student_performance_data), 0.5 * nrow(Student_performance_data))
StudentTrain <- Student_performance_data[inTrain,]
StudentTest <- Student_performance_data[-inTrain,]

# Visualize the distribution of GradeClass
ggplot(Student_performance_data, aes(x = GradeClass)) + geom_bar() + theme_minimal() + ggtitle("Distribution of Grade Classes")

# Visualize relationships between GPA and potential predictors
ggplot(Student_performance_data, aes(x = StudyTimeWeekly, y = GPA)) + geom_point() + theme_minimal() + ggtitle("Study Time Weekly vs GPA")

ggplot(Student_performance_data, aes(x = Age, y = GPA)) + geom_point() + theme_minimal() + ggtitle("Age vs GPA")

ggplot(Student_performance_data, aes(x = Gender, y = GPA)) + geom_boxplot() + theme_minimal() + ggtitle("Gender vs GPA")

# KNN
knn_pred_class <- knn(train = StudentTrain[, -which(names(StudentTrain) == "GradeClass")], 
                      test = StudentTest[, -which(names(StudentTest) == "GradeClass")], 
                      cl = StudentTrain$GradeClass, k = 3)
knn_pred_class <- factor(knn_pred_class, levels = levels(StudentTrain$GradeClass))

# Consistent levels
knn_pred_class <- factor(knn_pred_class, levels = levels(StudentTest$GradeClass))

# Confusion Matrix
knn_conf_matrix <- confusionMatrix(knn_pred_class, StudentTest$GradeClass)
print(knn_conf_matrix)

# Binary Sensitivity and Specificity Calculation
positive_class <- "1" # Adjust this according to your actual positive class label
knn_pred_class_binary <- knn_pred_class == positive_class
StudentTest_binary <- StudentTest$GradeClass == positive_class

sensitivity_knn <- sensitivity(as.factor(knn_pred_class_binary), as.factor(StudentTest_binary), positive = "TRUE")
specificity_knn <- specificity(as.factor(knn_pred_class_binary), as.factor(StudentTest_binary), positive = "TRUE")

sensitivity_knn
specificity_knn

# Support Vector Machine (SVM)
# SVMs are powerful for high-dimensional data classification.
svm_model <- svm(GradeClass ~ ., data = StudentTrain, kernel = "radial", probability = TRUE)
svm_pred_class <- predict(svm_model, StudentTest, probability = TRUE)
confusionMatrix(svm_pred_class, StudentTest$GradeClass)

# Ensure categorical variables are converted to factors
StudentTrain <- StudentTrain %>% mutate(across(where(is.character), as.factor))
StudentTest <- StudentTest %>% mutate(across(where(is.character), as.factor))

# One-hot encode categorical variables
dmy <- dummyVars(" ~ .", data = StudentTrain)
train_transformed <- data.frame(predict(dmy, newdata = StudentTrain))
test_transformed <- data.frame(predict(dmy, newdata = StudentTest))

# Convert the target variable to factor for XGBoost
train_transformed$GradeClass <- as.factor(StudentTrain$GradeClass)
test_transformed$GradeClass <- as.factor(StudentTest$GradeClass)

# Set up k-fold cross-validation using caret
train_control <- trainControl(method = "cv", number = 10)

# Define the parameter grid
xgb_grid <- expand.grid(nrounds = 100,  # Number of boosting rounds
                        max_depth = 3,  # Maximum depth of a tree
                        eta = 0.1,      # Learning rate
                        gamma = 0,      # Minimum loss reduction
                        colsample_bytree = 0.8,  # Subsample ratio of columns
                        min_child_weight = 1,    # Minimum sum of instance weight (hessian) needed in a child
                        subsample = 0.8)         # Subsample ratio of the training instances

# Train the XGBoost model with k-fold cross-validation (using caret)
xgb_model <- train(GradeClass ~ ., data = train_transformed,
                   method = "xgbTree",
                   trControl = train_control,
                   tuneGrid = xgb_grid,
                   metric = "Accuracy")

# Print the best model parameters
print(xgb_model$bestTune)

# Make predictions on the test set
xgb_pred_class <- predict(xgb_model, newdata = test_transformed)

# Convert predictions to factor with correct levels
xgb_pred_class <- factor(xgb_pred_class, levels = levels(StudentTrain$GradeClass))

# Evaluate the model
confusion_matrix_xgb <- confusionMatrix(xgb_pred_class, StudentTest$GradeClass)
print(confusion_matrix_xgb)

# Calculate binary sensitivity and specificity if needed
StudentTest_binary_xgb <- StudentTest$GradeClass == positive_class
xgb_pred_class_binary <- xgb_pred_class == positive_class

sensitivity_xgb <- sensitivity(as.factor(xgb_pred_class_binary), as.factor(StudentTest_binary_xgb), positive = "TRUE")
specificity_xgb <- specificity(as.factor(xgb_pred_class_binary), as.factor(StudentTest_binary_xgb), positive = "TRUE")

sensitivity_xgb
specificity_xgb

# Install and load necessary libraries if not already done
if (!requireNamespace("gbm", quietly = TRUE)) install.packages("gbm")
if (!requireNamespace("caret", quietly = TRUE)) install.packages("caret")
library(gbm)
library(caret)

# First, let's examine the current levels of GradeClass
print(levels(StudentTrain$GradeClass))

# Function to make valid R variable names
make_valid_names <- function(x) {
  make.names(as.character(x), unique = TRUE)
}

# Apply the function to both training and test datasets
StudentTrain$GradeClass <- factor(make_valid_names(StudentTrain$GradeClass))
StudentTest$GradeClass <- factor(make_valid_names(StudentTest$GradeClass), levels = levels(StudentTrain$GradeClass))

# Verify the new levels
print(levels(StudentTrain$GradeClass))

# Now proceed with the GBM modeling


# Set up k-fold cross-validation
set.seed(123)  # for reproducibility
k_folds <- 5  # number of folds
folds <- createFolds(StudentTrain$GradeClass, k = k_folds)

# Function to train GBM model with cross-validation
train_gbm_cv <- function(train_data, target_var, n_trees = 100, interaction_depth = 3, shrinkage = 0.1) {
  formula <- as.formula(paste(target_var, "~ ."))
  
  model <- train(
    formula,
    data = train_data,
    method = "gbm",
    trControl = trainControl(
      method = "cv",
      number = k_folds,
      classProbs = TRUE,
      savePredictions = "final"
    ),
    verbose = FALSE,
    tuneGrid = expand.grid(
      n.trees = n_trees,
      interaction.depth = interaction_depth,
      shrinkage = shrinkage,
      n.minobsinnode = 10
    )
  )
  
  return(model)
}

# Train the GBM model with cross-validation
gbm_model <- train_gbm_cv(StudentTrain, "GradeClass")

# Print model summary
print(gbm_model)

# Make predictions on the test set
gbm_pred_class <- predict(gbm_model, newdata = StudentTest)

# Ensure predictions have the same levels as the original GradeClass
gbm_pred_class <- factor(gbm_pred_class, levels = levels(StudentTest$GradeClass))

# Evaluate the model
confusion_matrix_gbm <- confusionMatrix(gbm_pred_class, StudentTest$GradeClass)
print(confusion_matrix_gbm)

# Calculate binary sensitivity and specificity
# Note: You may need to adjust the positive_class based on your new level names
positive_class <- levels(StudentTrain$GradeClass)[1]  # Adjust this if needed
StudentTest_binary <- StudentTest$GradeClass == positive_class
gbm_pred_class_binary <- gbm_pred_class == positive_class

sensitivity_gbm <- sensitivity(as.factor(gbm_pred_class_binary), as.factor(StudentTest_binary), positive = "TRUE")
specificity_gbm <- specificity(as.factor(gbm_pred_class_binary), as.factor(StudentTest_binary), positive = "TRUE")

print(paste("GBM Sensitivity:", sensitivity_gbm))
print(paste("GBM Specificity:", specificity_gbm))

# Variable Importance
var_imp <- varImp(gbm_model)
# print(var_imp)
plot(var_imp)

```